# ArenadataDB adb_fdw connector

#### Введение

- В этой статье рассматривается детали предлагаемой нами реализации коннектора для выполнения, так называемых, гетерогенных запросов в рамках одного кластера ArenadataDB и/или Greenplum. Этот коннектор позволяет объединять в запросах не только разные кластеры ADB, а и разные базы в рамках одного кластера, но при этом пользоваться возможностями установления соединений между сегментами.

#### Существующие подходы и решения

Суть предоставления возможности выполнения гетерогенных запросов - дать пользователю инструментарий осуществлять доступ к данным так, чтобы с точки зрения использования данных создавалось впечатление, будто бы они размещены в той же системе управления данными. При чем делать это по возможности максимально эффективно и прозрачно.

- Какой же инструментарий есть в нашем распоряжении?

##### Platform Extension Framework (PXF)

Первое, что приходит на ум - это распространенный среди пользователей PXF-сервис, фреймворк для объединения разных источников данных.

- К достоинствами можно отнести:

1. Проверенное временем решение на базе открытого исходного кода c возможностью доработки под свои потребности.
2. Доступный "из коробки" набор коннекторов к популярным источникам данных (Hadoop стек, доступные через JDBC источники данных, облачные хранилища).
3. Возможность настройки мониторинга.

- Но можно выделить и ряд недостатков:

1. Необходимость поддерживать обособленное решение на базе своего стека.
2. Выделение ресурсов, как правило, на тех же серверах, где и развернута сама СУБД.
3. Множественное преобразование и перекладывание одних и тех же данных на пути от представления в СУБД к типам, которыми оперирует сам PXF.

#### Расширение postgres\_fdw

Расширение `postgres_fdw` - фундамент, можно сказать источник вдохновения многих других существующих коннекторов.
Коннектор физически представляет собой расширения PostgreSQL, в случае Greenplum устанавливаемый на мастер хост.
Изначально расширение ориентировано на работу с удаленным PostgreSQL, но в рамках апстрима Greenplum доработано на работу в рамках распределенной СУБД. Взаимодействие организовано через стандартный протокол и библиотеку libpq, так что это позволяет прозрачно работать и с Greenplum (разумеется в стиле master-master).

![Alt text](pasted_0.png)

- На схеме выше локальный кластер (обозначенный синим цветом) представлен мастером и парой сегментов, удаленный кластер - мастер (обозначенный зеленым).

Для `SELECT` запроса схематично процесс взаимодействия выглядит примерно так:

1. Пользователь формирует `SELECT` запрос.
2. Запрос проходит стандартные этапы парсинга, анализа и планирования.
3. Расширение устанавливает соединение с удаленным мастером, открывает транзакцию и отправляет запрос (с учетом некоторых нюансов, что не все условия к примеру можно отправить на удаленный кластер).
4. Далее обрабатывается ответ и либо он возвращаетя пользователю, или, в случае более сложных запросов, результат обрабатывается как-то иначе.

Отличительной особенностью этой схемы является работа в стиле мастер-мастер. Сегменты в этом случае никак не задействованы.

В случае `INSERT` запросов всё также идёт через мастера, но задействуется механизм prepared выражений:


![Alt text](pasted_1.png)

- В чем плюсы для тех, кто решит воспользоваться для организации связи кластера расширением `postgres_fdw`:

1. Решение с открытым исходным кодом - собрал и поставил.
2. Работа в стиле мастер-мастер стабильна. Далее я поясню на примере `greenplum_fdw`, что под этим понимается.
3. Свежие версии коннектора, которые поддерживают новый интерфейс FDW умеют частично поддерживать push-down более сложных конструкций в виде соединений и т.п. Этой теме я уделю время в конце статьи.

- При этом есть очевидные минусы:

1. Поддержка параллельной работы отсутствует.
2. Если попробовать вызвать `DELETE` или `UPDATE` для таблицы на удаленном кластере, то коннектор будет возвращать ошибки.

##### Расширение greenplum\_fdw

От `postgres_fdw` перейдём к тяжелой артиллерии - к `greemplum_fdw`, входящем в платную enterprise версию Greenplum.  
И тут я хочу отметить, что в деле удалось поисследовать только бета версию, и, возможно, какие-то проблемы присущие бете были исправлены в последующих версиях.

 Что же не так с `greenplum_fdw`?

- Первое неудобство - это жестокое задание количества обработчиков. Допустим локальный кластер развернут на трёх primary-сегментах:

![Alt text](pasted_2.png)

- Допустим, удаленный Greenplum кластер при этом не совпадает по количеству сегментов - там их четыре.

![Alt text](pasted_3.png)

------------------------------------------
![Alt text](pasted_2.png)
![Alt text](pasted_4.png)
![Alt text](pasted_5.png)

- Если при объявлении внешней таблице на локальном кластере не задать нужно количество обработчиков (либо задать его таким числом, которое не совпадает с числом ремоут кластера), то запрос завершается ошибкой.

- Задать нужное количество обработчиков - проблема сама по себе не такая большая. Хотя запрашивать у владельцев другого кластера эту информацию не всегда может быть удобным. Хуже, что если конфигурация кластера поменяется, то запросы работать перестанут до момента, когда на локальном кластер не будет сделан `ALTER SERVER` и переконфигурирован на нужное число.

![Alt text](pasted_4.png)

Более неприятная ситуация может возникнуть в случае, если мы захотим перенести какие-то данные с локального кластера на удаленный путём вставки в foreign таблицу.

- Допустим на локальном кластере есть таблица в которой 1100 записей.

![Alt text](pasted_45.png)

- Для такого запроса вставки в foreign таблицу планировщик создаст примерно такой план, в котором внизу лежит `Seq Scan` узел плана (последовательное чтение данных из таблицы), который получает данные локальной таблицы. Далее делается `Redistribute Motion` данных между сегментами, далее идёт вставка с каждого сегмента по отдельности:

![Alt text](pasted_46.png)
![Alt text](pasted_47.png)

---------------------------

- В целях эксперимента добавим отображение шага коммита транзакции на уровне сегментов:

![Alt text](pasted_6.png)
![Alt text](pasted_61.png)
![Alt text](pasted_62.png)

-----------------

- Как мы видим с двух сегментов базе данных удалось закоммиттить транзакцию, но с третьей произошла некая ошибочная ситуация:

![Alt text](pasted_7.png)
![Alt text](pasted_71.png)
![Alt text](pasted_72.png)
![Alt text](pasted_73.png)

- В то время как мы делали один `INSERT` который по ожиданиям пользователя реляционной СУБД с поддержкой ACID должен быть атомарным, мы получаем 742 записи в удаленной таблице:

![Alt text](pasted_8.png)

![Alt text](pasted_81.png)

Предлагаю разобраться в чем причина такой ситуации.

- Смоделируем её следующим образом. Допустим у нас есть локальный кластер из трёх сегментов + мастер и удаленный кластер мастер и два сегмента.

![Alt text](pasted_9.png)

1. От пользователя приходит `INSERT` запрос .
2. Он проходит стадии обработки запроса и устанавливает соединения и открывает отдельные транзакции с сегментов.
3. Эти транзакции работают с мастером на удаленном кластере.

Тут важно понимать, что такая схема заложена в архитектуру `postgres_fdw`, который поддерживает пул соединений, в рамках которых при установлении соединения открывается транзакция. Она же и коммитится по отдельности не имея информации о статусе соседних. И в какой-то момент одна из них отваливается - мы получаем приведенный выше результат.

Как итоги, к достоинствам бета версии `greenplum_fdw` можно отнести поддержку `SELECT` в MPP-стиле, но возможные проблемы с консистентностью данных при вставке ставят под сомнения возможность использования коннектора как двунаправленного.

При этом у пользователя нет возможности управлять этими рисками - выбрав для вставки стратегию мастер к мастеру, так как в этом случае для `SELECT` запросов также будет такая же стратегию.

Да и `SELECT` запросы довольно жестко привязаны к конфигурациям по числу сегментов.

Ну и формальный минус, что по сравнению с `postgres_fdw` этот коннектор только в enterprise версии.

#### Parallel Retrieve Cursors

Для понимания внутреннего устройства `adb_fdw` коннектора нужно рассмотреть лежащий в его основе механизм параллельных курсоров. Курсоры, как известно, используются для получения данных построчно, в процедурном стиле. Параллельные курсоры в этом плане ничем не отличаются от своих собратьев. Они объявляются на координаторе запросов (в терминах Greenplum - `GP_ROLE_DISPATCH`), однако эндпоинты (о них далее) создаются на сегментах. Их размещение зависит от запроса, вернее от результата его планирования. Еще важно добавить, что для обслуживания эндпоинта - той точки, через которую происходит обмен данными, поднимается отдельный процесс бэкенда.

Рассмотрим три простых примера объявления параллельного курсора, которые приводят к разным конфигурациям размещения эндпоинтов.

##### Получения данных с мастера

К такой конфигурации эндпоинтов приводит необходимость собрать данные на мастере, например, как в данном случае для их сортировки:

![Alt text](pasted_10.png)

##### Получения данных с одного из сегментов

- Второй вариант - это получения данных от какого-то одного сегмента. Например, в случае если планировщик определяет, что в соответствии с ключом дистрибуции нужно задиспатчить план на какой-то определенный сегмент:

![Alt text](pasted_11.png)

##### Получения данных от всех сегментов

- и третий случай - это получения данных от всех сегментов. Например, получение всех колонок, без какой-либо фильтрации `WHERE` условиями . Эндоинты поднимаются на каждом сегменте и данные могут быть возвращены напрямую с сегментов:

![Alt text](pasted_12.png)

- Чуть подробнее погрузимся в механику процесса эндопоинтов на сегментах. Допустим у нас есть пользовательская сессия, в рамках которой на мастер пришел запрос. Он проходит обычную стадию парсинга и планирования:

![Alt text](pasted_13.png)

Далее план или какая-то часть плана диспетчеризируется на сегмент и обрабатывается в рамках процесса бэкенда.

В нашем случае, так как это запрос на создание параллельного курсора для лежащего в его основе `SELECT` запроса, то далее бэкенд создаёт необходимую обвязку для обслуживания канала получения данных клиентом.

Для этого помимо всего прочего в общей памяти поднимается очередь сообщений для обмена данным с тем самым отдельным процессом бэкенда, который используется клиентом для получения данных. С этой целью клиент поднимает отдельного, специальное `utility` соединение, в рамках которого допускается только извлечение данных параллельного курсора. На схеме выше этот соединение обозначено настройкой `gp_retrieve_conn = true`

***Именно этот механизм лежит в основе в `adb_fdw` коннектора.***

##### Коннектор ADB-to-ADB (adb\_fdw)

Для созданиях конкуретных преимуществ на текущем этапе развития нашем `adb_fdw` коннекторе были релизованы некоторые альтернативные технические решения.

Для начала посмотрим на него с пользовательской точки зрения.

Точкой входа во внешний кластер выступает определение `SERVER` - это мастер удаленного кластера. Ключевое слово `mpp_execute` заданное как `master` не означает, что все преимущества MPP для выполнения `SELECT` запросов сведены на нет. Наш коннектор сам определяет нужную конфигурацию обработчиков. Значение master необходимо, чтобы вставка работала только в режиме master - master.

Также можно задать число обработчиков, либо запросить коннектор самостоятельно определять число обработчиков на основе планирования запроса на удаленного кластере. Что является довольно удобным.

Далее, как обычно (для FDW-коннекторов) задаётся пользовательский маппинг и объявляется внешняя таблица:

![Alt text](pasted_14.png)

В чем особенности работы нашего коннектора?

##### SELECT запросы

![Alt text](pasted_15.png)


- Всё та же конфигурация кластера: три локальных сегмента и два удаленных. Пользователь делает запрос на `SELECT` трёх колонок без фильтров. Запрос в виде создания параллельного курсора прилетает на удаленный мастер. Далее удаленный мастер создаёт эндопоинты и возвращает локальному мастеру при вызове им функции `gp_get_endpoints()` служебную информацию по этим эндпоинтам.

- Далее, так как мы получили информацию, что эндпоинтов два, то нам будет достаточно двух обработчиков, которые и поднимаются в виде процессов бэкендов на локальном кластере (белые прямоугольники внутри зеленых на схеме выше). Сегменты поднимают служебное соединение - так называемую `retrieve` сессию и начинают параллельно получать данные напрямую с сегментов, отдавая их мастеру, так как в нашем случае этого требует запрос.

##### INSERT запросы

- На текущий момент не существует реализации надежного механизма обеспечить целостность вставки в режиме параллельной работы коннектора. В будущих версиях ядра можно ожидать такую поддержку, но готового механизма всё еще нет.

![Alt text](pasted_16.png)

В нашем коннекторе целостность поставлена во главу угла и поддерживается вставка в стиле `postgres_fdw` в режиме мастер-мастер.

#### Дальнейшие планы

В свежих версиях PostgreSQL и, как следствие, в Greenplum 7 интерфейс доступа к внешним источникам данных FDW расширился как несколькими новыми функциями, так и новыми параметрами существующих функций.

Суть эти изменений в предоставлении возможности поддержки push-down как соединений, так и агрегатных функций. Это выводит подобные коннекторы на новый уровень в плане возможности строить более оптимальные планы, сокращать объемы передаваемых данных и, в целом, ускорять выполнение запросов.

Рассмотрение этой интересной и объемной темы - предмет отдельной статьи. Сейчас хочется вкратце затронуть тему сложностей и подводных камней этих ожидаемых фич, которые мы планируем реализовать в следующих версиях конектора в рамках Arenadata DB версии 7x.

##### Ограничения предикатов

Не все предикаты можно передать на удаленный кластер. Есть ограничения по возможности передачи выражения с заданными правилами сортировки (`COLLATION`). Мотивацией такого типа ограничений связана с тем, что на удаленном кластере нельзя рассчитывать на такое же окружение, чтобы правила сортировки не по умолчанию сработали именно так, как на локальном кластере.

При явно заданных правилах сортировки (т.е. отличных от default collation с `OID = 100`) для отправки на удаленный кластер допускаются только выражения, которые связаны с объявленимя для внешней таблицы (foreign table). Таким образом, если правило сортировки для колонки явно задано при объявлении внешней таблицы, то это условие считается безопасным к отправке условием. Напротив, если для выражения выводится некоторое правило сортировки, отличное от default, то это считается не безопасным и обрабатывается локально. Например, для данного выражения сравнения колонки внешней таблицы с константой, для которой задано правило сортировки, фильтр применяется локально, уже после вычитывания всей таблицы:

```sql
adb=# explain select count(c1) from foreign_table as ft where ft.c1 like 'foo' COLLATE "POSIX";
                            QUERY PLAN                             
-------------------------------------------------------------------
 Aggregate  (cost=3897.97..3897.98 rows=1 width=8)
   ->  Foreign Scan on ft  (cost=100.00..3897.72 rows=100 width=16)
         Filter: (c1 ~~ 'foo'::text COLLATE "POSIX")
 Optimizer: Postgres-based planner
(4 rows)
```

В случае выведения правила по умолчанию происходит push-down на удаленный кластер:

```sql
adb=# explain select count(ft.c1) from foreign_table ft where ft.c1 like 'foo';
                      QUERY PLAN                      
------------------------------------------------------
 Foreign Scan  (cost=1894.20..1894.23 rows=1 width=8)
   Relations: Aggregate on (public.ft)
 Optimizer: Postgres-based planner
(3 rows)

```

Также не допускаются к отправке на удаленный кластер выражения с волатильными функциями (`VOLATILE`). Простейший пример функции `random()` или `now()`. Для таких запросов условие обрабатывается локально:

```sql
adb=# explain select * from foreign_table as ft where ft.c1 >= random();
                          QUERY PLAN                           
---------------------------------------------------------------
 Foreign Scan on ft  (cost=100.00..4398.60 rows=33392 width=16)
   Filter: ((c1)::double precision >= random())
 Optimizer: Postgres-based planner
```

##### Joins push-down

Возможность передать задачу соединения таблиц во внешний кластер (так называемый `join push-down`), если они фигурируют в запросе выглядит крайне полезной. Для начала по основным ограничениям:

1. Базовое требований - таблицы должны быть связаны с одним и тем же внешним сервером (имеется ввиду описание `SERVER`).
2. Join push-down на текущий момент поддерживается только для внутренних (`JOIN_INNER`), левых и правых внешних соединений (`JOIN_LEFT`, `JOIN_RIGHT`), а также полный соединений (`JOIN_FULL`). Дальнейшее развитие предполагает `SEMI-` и `ANTI-` соединения для поддержки SQL-конструкций вида `NOT IN` и `ANY`.
3. Внутренняя и внешняя таблицы соединения должны позволять передавать предикаты соединения, предикаты-фильтры выборки во внешний кластер. Если предикат не может беыть передан во внешний кластер, то push-down не возможен. Например:

```sql
adb=# explain select count(*) from ft_a inner join ft_b on ft_a.id = ft_b.id where ft_a.ts >= current_date::timestamp;
                          QUERY PLAN                          
--------------------------------------------------------------
 Foreign Scan  (cost=350.53..350.56 rows=1 width=8)
   Relations: Aggregate on ((public.ft_a) INNER JOIN (public.ft_b))
 Optimizer: Postgres-based planner
(3 rows)

```

В данном случае выражение `ft_a.ts >= current_date::timestamp` может быть преобразовано в константу, передано в таком виде на удаленный кластер и выступать условием выборки.

Однако, в случае такого запроса условие `ft_a.ts >= current_timestamp` не может быть передано, так как функция `current_timestamp` не является `IMMUTABLE` (а точнее является `STABLE`). Что приводит к двум отдельным `Foreign Scan` и локальному соединению и применениям условия (`Filter`):

```sql
adb=# explain select count(*) from ft_a inner join ft_b on ft_a.id = ft_b.id where ft_a.ts >= current_timestamp;
                                       QUERY PLAN                                        
-----------------------------------------------------------------------------------------
 Aggregate  (cost=664.92..664.93 rows=1 width=8)
   ->  Hash Join  (cost=239.04..658.24 rows=2670 width=0)
         Hash Cond: (ft_a.id = ft_b.id)
         ->  Foreign Scan on ft_a  (cost=100.00..480.00 rows=3333 width=12)
               Filter: (ts >= '2024-04-07 16:49:38.585287+03'::timestamp with time zone)
         ->  Hash  (cost=129.03..129.03 rows=801 width=4)
               ->  Foreign Scan on ft_b  (cost=100.00..129.03 rows=801 width=4)
 Optimizer: Postgres-based planner
(8 rows)

```

##### Aggregates push-down

- Для того, чтобы агрегатная функция могла быть передана для выполнения на удалённом кластере, должно выполняться несколько условий:

- Если в выборке используется предикат, который не может быть отправлен на удаленный кластер, то такая агрегация может быть осуществлена только локально, так условие предиката должно быть применено только в процессе выборки, а не после. Например, условие `ft_a.id >= random()` не позволяет сделать push down агрегатной функции `count(*)`, в то время как `ft_a.ts >= current_date::timestamp` позволяет: 
```sql
    adb=# explain select ft_a.name, count(*) from ft_a where ft_a.ts >= current_date::timestamp group by ft_a.name;
                         QUERY PLAN                      
    -----------------------------------------------------
     Foreign Scan  (cost=330.00..330.03 rows=1 width=40)
       Relations: Aggregate on (public.ft_a)
     Optimizer: Postgres-based planner
    (3 rows)
    
    adb=# explain select ft_a.name, count(*) from ft_a where ft_a.id >= random() group by ft_a.name;
                                 QUERY PLAN                              
    ---------------------------------------------------------------------
     HashAggregate  (cost=546.67..556.67 rows=1000 width=40)
       Group Key: name
       ->  Foreign Scan on ft_a  (cost=100.00..530.00 rows=3333 width=8)
             Filter: ((id)::double precision >= random())
     Optimizer: Postgres-based planner
    (5 rows)
```
- Это должна быть либо простая агрегатная функция, например `count`, `min`, `max` и т.п., либо функция, поддерживающая частичную (partial aggregation). Также, с точки зрения контекста выполнения функции частичной агрерации это должна быть начальная фазы частичной агрегации.
- Для второго случая частичной агрегации должны выполняться условия: 
    - агрегатная функция не должна быть сортирующей (Ordered-Set Aggregate Functions не поддерживаются). В эту категорию попадают процентили (percentile), так как для их вычисления требуется упорядоченный набор значений;
    - для агрегатной функции должно отсутствовать определение финальной функции (`aggfinalfn`), и таким образом её применение не потребует фазы вычисления результирующего значения;
    - если все-таки `aggfinalfn` объявлена вместе с функцией `aggcombinefn`, то поддерживается только определенный набор таких функций (в основном это вариации `avg` и `sum`).
- Также, не поддерживаются выражения c `GROUPING SETS`.

В чем же смысл подобных ограничений? Для распределенных СУБД вычисление простых агрегатов не представляет особых сложностей. Отдельные значения можно подсчитать на сегментах и в результирующей функции произвести финальный подсчёт. Несложно прикинуть такое разбиение на подзадачи для `min`, `max`, `count` и т.п. функций.

С фукнцией `avg` уже интереснее - для `avg` при возврате данных от сегмента помимо вычисленного среднего значения требуется и общее число строк, для которых это среднее было рассчитано. Только так финальная функция сможет рассчитать итоговое значение. По этой причине `avg` может быть диспетчеризирована на сегменты в виде "`array[count(), sum()]`" выражения (для чисел с плавающей точкой выражение выглядит чуть сложнее).

##### Итоги

По сравнению конкурирующими решениями у коннектора `adb_fdw` есть ряд преимуществ, которые выгодно отличают его от рассмотренных альтернатив:

1. Также, как и `greenplum_fdw` наше решение поддерживает параллельную работу через сегменты в `SELECT` запросах, там где запросы это позволяют делать, но делает это в более удобном для пользователя варианте - отсутствует необходимость отслеживать число обработчиков.
2. Взаимодействие master-master при `INSERT` запросах позволяет нам не натыкаться на те проблемы целостности данных, которые могут встречаться в `greenplum_fdw`.
3. Также в `adb-fdw` более аккуратно обрабатывается ситуация с отсутствием на текущий момент поддержки `DELETE` и `UPDATE` запросов.

Внедрение поддержи push-down функций агрегатов и соединений в следующей версии коннектора `adb_fdw` еще больше расширит области его применения, сократит объемы передаваемых данных и увеличит скорость взаимодействия в наиболее распространенных сценариях использования.
